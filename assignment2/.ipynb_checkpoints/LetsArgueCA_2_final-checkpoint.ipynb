{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09ba900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "\n",
    "df = pd.read_csv(r'E:/germany_details/Uni-assist LOM/paderborn admit/sem1/Computational Argumentation/assignment2/data/train-bio.csv', sep='\\t', header=None)\n",
    "df.columns = ['tokens','BIO_Label']\n",
    "\n",
    "vc = df['BIO_Label'].value_counts()\n",
    "index = list(vc.index)\n",
    "count = min(vc.values)\n",
    "\n",
    "df_bal = pd.DataFrame()\n",
    "for i in index:\n",
    "    temp = df[df['BIO_Label']==i].sample(count)\n",
    "    df_bal = df_bal.append(temp, ignore_index = True)\n",
    "    \n",
    "df = df_bal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e724fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 VERB 1685\n",
      "95 PRON 2192\n",
      "84 ADJ 1260\n",
      "92 NOUN 3938\n",
      "97 PUNCT 1092\n",
      "86 ADV 839\n",
      "85 ADP 1357\n",
      "94 PART 255\n",
      "89 CCONJ 297\n",
      "90 DET 1084\n",
      "87 AUX 664\n",
      "96 PROPN 342\n",
      "98 SCONJ 504\n",
      "93 NUM 104\n",
      "101 X 10\n",
      "91 INTJ 25\n",
      "99 SYM 1\n",
      "103 SPACE 38\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(name='en_core_web_sm')\n",
    "\n",
    "# words = [str(x) for x in df['tokens']]\n",
    "# doc = nlp(' '.join(words))\n",
    "\n",
    "# for key, val in doc.count_by(spacy.attrs.POS).items():\n",
    "#     print(key, doc.vocab[key].text, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df28b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>BIO_Label</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>place</td>\n",
       "      <td>I</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unpleasant</td>\n",
       "      <td>I</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its</td>\n",
       "      <td>I</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>I</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deny</td>\n",
       "      <td>I</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tokens BIO_Label pos_tags\n",
       "0       place         I      NNS\n",
       "1  unpleasant         I      NNS\n",
       "2         its         I      NNS\n",
       "3        like         I      NNS\n",
       "4        deny         I      NNS"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['pos_tags'] = 'X'\n",
    "\n",
    "# for token,biolabel,pos_tags in df.itertuples(index=False):\n",
    "#     doc = nlp(str(token))\n",
    "#     for word in doc:\n",
    "#         df['pos_tags'] = word.tag_\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2662f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.08      0.04      0.05        25\n",
      "           I       0.73      0.87      0.79       396\n",
      "           O       0.59      0.39      0.47       179\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.47      0.43      0.44       600\n",
      "weighted avg       0.66      0.69      0.66       600\n",
      "\n",
      "   B    I   O\n",
      "B  1   20   4\n",
      "I  9  343  44\n",
      "O  3  107  69\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True) #shuffle your dataframe in-place and reset the index, only sample will also work\n",
    "\n",
    "df_shuffled = df_shuffled.iloc[:3000]\n",
    "\n",
    "for token,biolabel in df_shuffled.itertuples(index=False):\n",
    "    if str(token) not in stopwords.words('english'):\n",
    "        token = re.sub(r'\\W', ' ', str(token)) #replace all non word char with space\n",
    "        token = token.lower()\n",
    "        #token = re.sub(r'\\s+[a-z]\\s+', ' ',token) #substitute all single characters with space on l&r\n",
    "        #token = re.sub(r'\\s+', ' ', token)  #substitute all spaces to single space\n",
    "\n",
    "        \n",
    "# we have to remove empty token which are being generated by substituting\n",
    "df_shuffled = df_shuffled.replace(r'^s*$', float('NaN'), regex = True)\n",
    "df_shuffled.dropna(inplace = True)\n",
    "\n",
    "# df_shuffled.isnull().sum()\n",
    "# df_shuffled.dropna(inplace=True)\n",
    "\n",
    "def get_vec(x):\n",
    "    doc = nlp(str(x))\n",
    "    vec = doc.vector\n",
    "    return vec\n",
    "\n",
    "df_shuffled['vec'] = df_shuffled['tokens'].apply(lambda x: get_vec(x))\n",
    "\n",
    "X = df_shuffled['vec'].to_numpy()\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, 300)\n",
    "\n",
    "y = df_shuffled['BIO_Label']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
    "X_words = vectorizer.fit_transform(df_shuffled['tokens']).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# clf = LinearSVC()\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#if C>1 means we will regularize, else keep 1\n",
    "# svm_model_linear = SVC(kernel = 'rbf', C = 10, gamma='auto').fit(X_train, y_train)\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1, gamma='auto').fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)\n",
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "print(metrics.classification_report(y_test, svm_predictions))\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "\n",
    "# dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
    "# dtree_predictions = dtree_model.predict(X_test)\n",
    "# accuracy = dtree_model.score(X_test, y_test)\n",
    "# print(accuracy)\n",
    "# print(metrics.classification_report(y_test, dtree_predictions))\n",
    "# cm = confusion_matrix(y_test, dtree_predictions)\n",
    "\n",
    "# naive_bayes_model = GaussianNB().fit(X_train, y_train)\n",
    "# naive_bayes_predictions = naive_bayes_model.predict(X_test)\n",
    "# accuracy = naive_bayes_model.score(X_test, y_test)\n",
    "# print(accuracy)\n",
    "# print(metrics.classification_report(y_test, naive_bayes_predictions))\n",
    "# cm = confusion_matrix(y_test, naive_bayes_predictions)\n",
    "\n",
    "df2 = pd.DataFrame(cm,index=['B','I','O'], columns=['B','I','O'])\n",
    "print(df2)\n",
    "\n",
    "# Macro F1-Score: 0.541\n",
    "# Weighted F1-Score: 0.71\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a92740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# having more parameters will take days to get best parameters so remove few\n",
    "# hyperparameters = [\n",
    "#     {'kernel': ['rbf'],\n",
    "#      'gamma': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "#      'C': [1, 10, 100, 500, 1000],\n",
    "#      'degree': [2, 3, 4, 5]},\n",
    "                   \n",
    "#     {'kernel': ['linear'],\n",
    "#      'C': [1, 10, 100, 500, 1000]}]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "hyperparameters = [\n",
    "    {'kernel': ['rbf'],\n",
    "     'gamma': [1e-1, 1e-2],\n",
    "     'C': [1, 10, 100],\n",
    "     'degree': [2, 3]},                 \n",
    "    {'kernel': ['linear'],\n",
    "     'C': [1, 10, 100]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "def run_tuning(model, hyperparameters, scores):\n",
    "    for score in scores:\n",
    "        print(\"Tuning hyperparamters for %s\" % score)\n",
    "        print()     \n",
    "        clf = GridSearchCV(model, hyperparameters, scoring='%s_macro' % score, cv = 5, n_jobs = -1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Best parameters set found: ')\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print('Grid scores in process: ')\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        for mean, params in zip(means, clf.cv_results_['params']):\n",
    "            print('%0.3f for %r' % (mean, params))\n",
    "            \n",
    "        print()\n",
    "        print()\n",
    "        print('Detailed Classification Report')\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb309b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "hyperparameters = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10, 100],\n",
    "    'max_features': [2, 3, X.shape[1]],\n",
    "    'min_samples_leaf': [2, 5],\n",
    "    'n_estimators': [10, 100, 200]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5214ac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparamters for precision\n",
      "\n",
      "Best parameters set found: \n",
      "\n",
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "\n",
      "Grid scores in process: \n",
      "\n",
      "0.534 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.552 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.571 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.539 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.574 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.543 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.564 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.564 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.529 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.565 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.576 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.549 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.568 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.555 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.568 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.568 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.534 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.562 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.570 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.545 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.563 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.539 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.565 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.567 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.529 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.564 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.527 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.560 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.569 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.546 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.563 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.572 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "Detailed Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.67      0.69      0.68       206\n",
      "           I       0.48      0.57      0.52       193\n",
      "           O       0.53      0.41      0.46       201\n",
      "\n",
      "    accuracy                           0.56       600\n",
      "   macro avg       0.56      0.56      0.56       600\n",
      "weighted avg       0.56      0.56      0.56       600\n",
      "\n",
      "\n",
      "Tuning hyperparamters for recall\n",
      "\n",
      "Best parameters set found: \n",
      "\n",
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "\n",
      "Grid scores in process: \n",
      "\n",
      "0.524 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.570 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.563 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.536 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.556 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.569 for {'bootstrap': True, 'max_depth': 10, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.541 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.560 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.562 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.544 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.564 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.568 for {'bootstrap': True, 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.545 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.561 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.570 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.545 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.572 for {'bootstrap': True, 'max_depth': 10, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.541 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.560 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.560 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.546 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.562 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.567 for {'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.542 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.567 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.566 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.536 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.563 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.564 for {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "0.558 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "0.562 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "0.569 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "0.542 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.560 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "0.567 for {'bootstrap': True, 'max_depth': 100, 'max_features': 300, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "Detailed Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.65      0.71      0.68       206\n",
      "           I       0.47      0.54      0.50       193\n",
      "           O       0.48      0.36      0.41       201\n",
      "\n",
      "    accuracy                           0.54       600\n",
      "   macro avg       0.53      0.54      0.53       600\n",
      "weighted avg       0.53      0.54      0.53       600\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run_tuning(SVC(), hyperparameters, scores)\n",
    "run_tuning(rfc, hyperparameters, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b09fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here we will predict for test-bio\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_abc = pd.read_csv(r'E:/germany_details/Uni-assist LOM/paderborn admit/sem1/Computational Argumentation/assignment2/data/test-bio.csv', sep='\\t', header=None)\n",
    "df_abc.columns = ['tokens','BIO_Label']\n",
    "\n",
    "y_test = df_abc['BIO_Label']\n",
    "\n",
    "def get_vec(x):\n",
    "    doc = nlp(str(x))\n",
    "    vec = doc.vector\n",
    "    return vec\n",
    "\n",
    "df_abc['vec'] = df_abc['tokens'].apply(lambda x: get_vec(x))\n",
    "X = df_abc['vec'].to_numpy()\n",
    "X = X.reshape(-1, 1)\n",
    "X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, 2851104)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 300)\n",
    "X_words = tfidf.fit_transform([str(x) for x in df_abc['tokens']]).toarray()\n",
    "\n",
    "pred = svm_model_linear.predict(X)\n",
    "print(pred)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346071f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
